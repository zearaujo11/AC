{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of teams that will reach the Playoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "! pip install tabulate\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "players = pd.read_csv('basketballPlayoffs/players.csv', delimiter=\",\")\n",
    "coaches = pd.read_csv('basketballPlayoffs/coaches.csv', delimiter=\",\")\n",
    "teams = pd.read_csv('basketballPlayoffs/teams.csv', delimiter=\",\")\n",
    "players_teams = pd.read_csv('basketballPlayoffs/players_teams.csv', delimiter=\",\")\n",
    "teams_post = pd.read_csv('basketballPlayoffs/teams_post.csv', delimiter=\",\")\n",
    "series_post = pd.read_csv('basketballPlayoffs/series_post.csv', delimiter=\",\")\n",
    "awards_players = pd.read_csv('basketballPlayoffs/awards_players.csv', delimiter=\",\")\n",
    "awards_coaches = pd.read_csv('basketballPlayoffs/awards_coaches.csv', delimiter=\",\")\n",
    "\n",
    "print(players.head())\n",
    "print(coaches)\n",
    "print(teams)\n",
    "print(players_teams)\n",
    "print(teams_post)\n",
    "print(series_post)\n",
    "print(awards_players)\n",
    "print(awards_coaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values, outliers, and inconsistencies in the data. Clean and preprocess the data to ensure it's ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams.isna().sum())\n",
    "print(teams.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players Teams dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players_teams.isna().sum())\n",
    "print(players_teams.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players.isna().sum())\n",
    "print(players.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coaches dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coaches.isna().sum())\n",
    "print(coaches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awards Players dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(awards_players.isna().sum())\n",
    "print(awards_players.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awards Coaches dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(awards_coaches.isna().sum())\n",
    "print(awards_coaches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct EDA to gain insights into the data. Visualize distributions, correlations, and patterns. This step will help you understand the relationships between different features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams.head()\n",
    "\n",
    "teams.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in teams.columns:\n",
    "    unique_values = teams[column].unique()\n",
    "    print(f\"Number of different values in the {column} column are:\", len(unique_values))\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "value_counts = []\n",
    "\n",
    "for column in teams.columns:\n",
    "    unique_values = teams[column].nunique()\n",
    "    columns.append(column)\n",
    "    value_counts.append(unique_values)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(columns, value_counts, color='skyblue')\n",
    "plt.xlabel('Number of Unique Values')\n",
    "plt.ylabel('Columns')\n",
    "plt.title('Number of Unique Values in Each Column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_numeric = teams.copy()\n",
    "\n",
    "for column in teams_numeric.columns:\n",
    "    if teams_numeric[column].dtype == 'object':\n",
    "        teams_numeric[column] = teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "teams_numeric.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(dataframe):\n",
    "    corr_matrix = dataframe.corr()\n",
    "\n",
    "    target_correlation = corr_matrix['playoff']\n",
    "\n",
    "    plt.figure(figsize=(30, 20))\n",
    "\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, annot_kws={\"size\": 8}, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
    "\n",
    "    plt.title('Correlation Matrix', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    dict = {}\n",
    "\n",
    "    for feature, correlation in target_correlation.items():\n",
    "        print(f\"Correlation between target and {feature}: {correlation}\")\n",
    "        dict[feature] = correlation\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(teams_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square(dataset, target): \n",
    "    for feature in dataset.columns:\n",
    "        if feature != target:\n",
    "            contingency_table = pd.crosstab(dataset[feature], dataset[target])\n",
    "\n",
    "            # check if any category has no data\n",
    "            if contingency_table.shape[0] == 0 or contingency_table.shape[1] == 0:\n",
    "                print(f\"No data for {feature} and {target}\")\n",
    "                continue\n",
    "            \n",
    "            chi2, p, observed, expected = chi2_contingency(contingency_table)\n",
    "            \n",
    "            # Step 4: Print or store the results\n",
    "            print(f\"Chi-square test for {feature} and {target}:\")\n",
    "            print(f\"Chi-square value: {chi2}\")\n",
    "            print(f\"P-value: {p}\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square(teams, 'playoff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value indicates the probability of observing a relationship as extreme as the one in our sample data, assuming that there is no actual relationship in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players['playerID'].nunique()) \n",
    "\n",
    "print(players.head())\n",
    "\n",
    "players.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erased columns and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped columns: 'collegeOther', 'deathDate', 'firstseason', 'lastseason'\n",
    "\n",
    "# Count the rows where 'firstseason' is not equal to 0\n",
    "non_zero_firstseason_count = len(players[players['firstseason'] != 0])\n",
    "\n",
    "# Count the rows where 'firstseason' is not equal to 0\n",
    "non_zero_lastseason_count = len(players[players['lastseason'] != 0])\n",
    "\n",
    "# Count the rows where 'deathDate' is not equal to \"0000-00-00\"\n",
    "players['deathDate'] = players['deathDate'].str.strip()\n",
    "non_empty_deathDate_count = len(players[players['deathDate'] != \"0000-00-00\"])\n",
    "\n",
    "# Count the rows where 'collegeOther' is not equal to \"\"\n",
    "non_nan_collegeOther_count = players['collegeOther'].notna().sum()\n",
    "\n",
    "print(\"Number of rows with 'firstseason' different from 0:\", non_zero_firstseason_count)\n",
    "print(\"Number of rows with 'lastseason' different from 0:\", non_zero_lastseason_count)\n",
    "print(\"Number of rows with 'collegeOther' different from \"\":\", non_nan_collegeOther_count)\n",
    "print(\"Number of rows with 'deathDate' different from '0000-00-00':\", non_empty_deathDate_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Players heights comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert height from inches to centimeters\n",
    "players['height_cm'] = players['height'] * 2.54  # 1 inch = 2.54 cm\n",
    "\n",
    "# Define height categories in centimeters\n",
    "height_categories = ['< 160.0 cm', '160.0 - 170.0 cm', '170.0 - 180.0 cm', '180.0 - 190.0 cm', '190.0 - 200.0 cm', '> 200.0 cm']\n",
    "\n",
    "# Define the height ranges for each category\n",
    "height_ranges = [(0, 160.0), (160.0, 170.0), (170.0, 180.0), (180.0, 190.0), (190.0, 200.0), (200.0, float('inf'))]\n",
    "\n",
    "# Create a new column in the dataset to store the height category for each player\n",
    "players['height_category'] = pd.cut(players['height_cm'], bins=[r[0] for r in height_ranges] + [float('inf')], labels=height_categories)\n",
    "\n",
    "# Count the number of players in each height category\n",
    "height_category_counts = players['height_category'].value_counts().reindex(height_categories, fill_value=0)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=height_category_counts.index, y=height_category_counts.values, palette='Set2')\n",
    "\n",
    "# Add labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Player Count in Height Categories')\n",
    "plt.xlabel('Height Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of players in each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot for player positions\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "ax = sns.countplot(data=players, x='pos', order=players['pos'].value_counts().index, palette='Set2')\n",
    "\n",
    "# Add labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Number of Players in Each Position')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Rotate x-axis labels for better readability (optional)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "empty_pos_count = players['pos'].isnull().sum()\n",
    "print(\"Number of rows with empty 'pos':\", empty_pos_count)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 Colleges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 colleges with the most players\n",
    "top_10_colleges = players['college'].value_counts().iloc[:10]\n",
    "\n",
    "# Create a countplot for the top 10 colleges\n",
    "plt.figure(figsize=(12, 6))  # Adjust the figure size as needed\n",
    "ax = sns.countplot(data=players, x='college', order=top_10_colleges.index, palette='Set2')\n",
    "\n",
    "# Add labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Top 10 Colleges with the Most Players')\n",
    "plt.xlabel('College')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Rotate x-axis labels for better readability (optional)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix between numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns for correlation analysis\n",
    "numeric_columns = [\"firstseason\", \"lastseason\", \"height\", \"weight\"]\n",
    "\n",
    "# Create a subset of the dataset with only the numeric columns\n",
    "subset = players[numeric_columns]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = subset.corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams Post metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams_post.head())\n",
    "\n",
    "teams_post.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate win-loss ratios\n",
    "teams_post['Win-Loss Ratio'] = teams_post['W'] / (teams_post['W'] + teams_post['L'])\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(teams_post['tmID'], teams_post['Win-Loss Ratio'], color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Win-Loss Ratio')\n",
    "plt.ylabel('Team ID (tmID)')\n",
    "plt.title('Win-Loss Ratios for Teams on  Post-Season (based on tmID)')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Post metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by year and count the number of wins for each team in each year\n",
    "team_wins_by_year = series_post.groupby(['year', 'tmIDWinner'])['W'].count().unstack(fill_value=0)\n",
    "\n",
    "# Create a stacked bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "team_wins_by_year.plot(kind='bar', stacked=True, colormap='Set3')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Wins')\n",
    "plt.title('Teams That Won In The Playoffs Each Year')\n",
    "\n",
    "# Show the chart\n",
    "plt.legend(title='Team', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teams that won and lost each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the \"F\" (Finals) round\n",
    "finals_data = series_post[series_post['round'] == 'F']\n",
    "\n",
    "# Create a DataFrame with the winning and losing teams for each year\n",
    "finals_results = finals_data[['year', 'tmIDWinner', 'tmIDLoser']]\n",
    "\n",
    "# Convert the DataFrame to a prettily formatted table\n",
    "table = tabulate(finals_results, headers='keys', tablefmt='fancy_grid', showindex=False)\n",
    "\n",
    "# Display the formatted table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the \"F\" (Finals) round\n",
    "finals_data = series_post[series_post['round'] == 'F']\n",
    "\n",
    "# Count how many times each team has appeared in the Finals as either a winner or a loser\n",
    "team_appearances = pd.concat([finals_data['tmIDWinner'], finals_data['tmIDLoser']]).value_counts()\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "team_appearances.plot(kind='barh', color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Team (tmID)')\n",
    "plt.xlabel('Number of Finals Appearances')\n",
    "plt.title('Total Appearances of Each Team in the Finals')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coaches metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coaches['coachID'].nunique()) \n",
    "\n",
    "print(coaches.head())\n",
    "\n",
    "coaches.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the wins and losses data from the \"won\" and \"lost\" columns\n",
    "coach_wins = coaches['won']\n",
    "coach_losses = coaches['lost']\n",
    "\n",
    "# Create a scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(coach_wins, coach_losses, alpha=0.5)\n",
    "plt.xlabel('Wins')\n",
    "plt.ylabel('Losses')\n",
    "plt.title('Scatter Plot of Coach Wins vs. Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify and select relevant features for your prediction model. Use techniques such as correlation analysis, recursive feature elimination, or feature importance from tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_teams = teams.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete: lgID, divID, seeded, tmORB, tmDRB, tmTRB, opptmORB, opptmDRB, opptmTRB, rank, firstRound, semis, finals\n",
    "print(teams.isna().sum())\n",
    "   \n",
    "feature_selection_result = teams.drop(columns=['lgID', 'divID', 'seeded', 'tmORB', 'tmDRB', 'tmTRB', 'opptmORB', 'opptmDRB', 'opptmTRB'])\n",
    "feature_selection_result = feature_selection_result.drop(columns=['rank', 'firstRound', 'semis', 'finals'])\n",
    "feature_selection_result = feature_selection_result.drop(columns=['min', 'o_oreb', 'o_dreb', 'd_oreb', 'd_dreb', 'name', 'franchID'])\n",
    "\n",
    "feature_selection_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_result.to_csv('filtered/feature_selection_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new features that might enhance the predictive power of your model. This could involve transforming existing features, creating interaction terms, or incorporating external data.\n",
    "\n",
    "The feauture enginnering is done inside the `players.ipynb` file and the creation of the new dataset is done inside `create_final_team.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_target_variable(dataset):\n",
    "    dataset.sort_values(by=['tmID', 'year'], inplace=True)\n",
    "\n",
    "    dataset['playoffs'] = dataset.groupby('tmID')['playoff'].shift(-1)\n",
    "\n",
    "    dataset.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    dataset.dropna(subset=['playoffs'], inplace=True)\n",
    "\n",
    "    dataset.rename(columns={'playoffs': 'playoff'}, inplace=True)\n",
    "\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_teams = shift_target_variable(original_teams)\n",
    "original_teams.to_csv('filtered/original_teams.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_teams_numeric = original_teams.copy()\n",
    "\n",
    "for column in original_teams_numeric.columns:\n",
    "    if original_teams_numeric[column].dtype == 'object':\n",
    "        original_teams_numeric[column] = original_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(original_teams_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_result = shift_target_variable(feature_selection_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_teams_numeric = feature_selection_result.copy()\n",
    "\n",
    "for column in feature_teams_numeric.columns:\n",
    "    if feature_teams_numeric[column].dtype == 'object':\n",
    "        feature_teams_numeric[column] = feature_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(feature_teams_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: verificar se pode ficar assim ou se adicionamos o código cá\n",
    "feature_engineering_result = pd.read_csv('filtered/feature_engineering_dataset.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng1_teams_numeric = shift_target_variable(feature_engineering_result.copy())\n",
    "eng1_teams_numeric.to_csv('filtered/eng1_teams_numeric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng1_teams_numeric = eng1_teams_numeric.copy()\n",
    "\n",
    "for column in eng1_teams_numeric.columns:\n",
    "    if eng1_teams_numeric[column].dtype == 'object':\n",
    "        eng1_teams_numeric[column] = eng1_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(eng1_teams_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_result2 = pd.read_csv('filtered/team2_before_shift.csv', delimiter=\",\")\n",
    "feature_engineering_result2.drop(columns=['playoff'], inplace=True)\n",
    "copy_fe1 = feature_engineering_result.copy()\n",
    "copy_fe1.drop(columns=['powerRanking2'], inplace=True)\n",
    "\n",
    "feature_engineering_result2 = pd.merge(feature_engineering_result2, copy_fe1, on=['tmID', 'year'])\n",
    "feature_engineering_result2.to_csv('text.csv', index=False)\n",
    "feature_engineering_result2 = shift_target_variable(feature_engineering_result2)\n",
    "feature_engineering_result = shift_target_variable(feature_engineering_result)\n",
    "\n",
    "#feature_engineering_result2.to_csv('text.csv', index=False)\n",
    "\n",
    "feature_engineering_result2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2_teams_numeric = feature_engineering_result2.copy()\n",
    "\n",
    "for column in eng2_teams_numeric.columns:\n",
    "    if eng2_teams_numeric[column].dtype == 'object':\n",
    "        eng2_teams_numeric[column] = eng2_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(eng2_teams_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = ['year', 'average_powerRanking', 'average_PER', 'average_postPowerRanking', 'average_postPER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2_teams_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmID, confID, playoff, arena\n",
    "# name, franchID, lgID, divID\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to the columns 'teamID', 'franchID', 'confID', 'name', 'arena'\n",
    "original_teams['tmID'] = label_encoder.fit_transform(original_teams['tmID'])\n",
    "original_teams['confID'] = label_encoder.fit_transform(original_teams['confID'])\n",
    "original_teams['arena'] = label_encoder.fit_transform(original_teams['arena'])\n",
    "original_teams['name'] = label_encoder.fit_transform(original_teams['name'])\n",
    "original_teams['franchID'] = label_encoder.fit_transform(original_teams['franchID'])\n",
    "original_teams['lgID'] = label_encoder.fit_transform(original_teams['lgID'])\n",
    "original_teams['divID'] = label_encoder.fit_transform(original_teams['divID'])\n",
    "original_teams['firstRound'] = label_encoder.fit_transform(original_teams['firstRound'])\n",
    "original_teams['semis'] = label_encoder.fit_transform(original_teams['semis'])\n",
    "original_teams['finals'] = label_encoder.fit_transform(original_teams['finals'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_result['tmID'] = label_encoder.fit_transform(feature_selection_result['tmID'])\n",
    "feature_selection_result['confID'] = label_encoder.fit_transform(feature_selection_result['confID'])\n",
    "feature_selection_result['arena'] = label_encoder.fit_transform(feature_selection_result['arena'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to the columns 'teamID'\n",
    "feature_engineering_result['tmID'] = label_encoder.fit_transform(feature_engineering_result['tmID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_result2['tmID'] = label_encoder.fit_transform(feature_engineering_result2['tmID'])\n",
    "feature_engineering_result2['confID'] = label_encoder.fit_transform(feature_engineering_result2['confID'])\n",
    "\"\"\" feature_engineering_result2['arena'] = label_encoder.fit_transform(feature_engineering_result2['arena'])\n",
    "feature_engineering_result2['firstRound'] = label_encoder.fit_transform(feature_engineering_result2['firstRound'])\n",
    "feature_engineering_result2['semis'] = label_encoder.fit_transform(feature_engineering_result2['semis'])\n",
    "feature_engineering_result2['finals'] = label_encoder.fit_transform(feature_engineering_result2['finals']) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, scoring=None, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=scoring, train_sizes=train_sizes, n_jobs=-1\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def decision_tree_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    # Define the hyperparameters and their possible values\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 2, 3, 4, 5, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    dt_classifier = DecisionTreeClassifier()\n",
    "    grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Step 2: Extract the best parameters from Grid Search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Step 3: Create a Decision Tree classifier with the best parameters\n",
    "    best_model = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "    # Step 4: Perform Recursive Feature Elimination (RFE)\n",
    "    rfe = RFE(estimator=best_model, n_features_to_select=1)\n",
    "    X_rfe = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "    # Step 5: Train the final Decision Tree model with the best parameters and selected features\n",
    "    best_model.fit(X_rfe, y_train)\n",
    "\n",
    "    # Step 6: Evaluate the model on the test set\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    y_pred = best_model.predict(X_test_rfe)\n",
    "\n",
    "    title = \"Learning Curves (Decision Tree)\"  # Change the title accordingly\n",
    "    scoring = \"accuracy\"  # Change to your preferred scoring metric\n",
    "\n",
    "    # Assuming you have a function named plot_learning_curve\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,  # Number of cross-validation folds\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def random_forest_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    # Define the hyperparameters and their possible values\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10, 20, 30],\n",
    "        'min_samples_leaf': [1, 2, 5, 10], \n",
    "        'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Step 2: Extract the best parameters from Grid Search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Step 3: Create a Random Forest classifier with the best parameters\n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "\n",
    "    # Step 4: Perform Recursive Feature Elimination (RFE)\n",
    "    rfe = RFE(estimator=best_model, n_features_to_select=1)\n",
    "    X_rfe = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "    # Step 5: Train the final Random Forest model with the best parameters and selected features\n",
    "    best_model.fit(X_rfe, y_train)\n",
    "\n",
    "    # Step 6: Evaluate the model on the test set\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    y_pred = best_model.predict(X_test_rfe)\n",
    "\n",
    "    title = \"Learning Curves (Random Forest)\"  # Change the title accordingly\n",
    "    scoring = \"accuracy\"  # Change to your preferred scoring metric\n",
    "\n",
    "    # Assuming you have a function named plot_learning_curve\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,  # Number of cross-validation folds\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4, 10): \n",
    "    print(\"Year: \", i)\n",
    "    random_forest_model(feature_engineering_result2, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1, 8): \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logistic_regression_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    # Define the hyperparameters and their possible values\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'lbfgs', 'saga'],\n",
    "        'max_iter': [1000, 10000]\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    lr_classifier = LogisticRegression()\n",
    "    grid_search = GridSearchCV(estimator=lr_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Step 2: Extract the best parameters from Grid Search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Step 3: Create a Logistic Regression classifier with the best parameters\n",
    "    best_model = LogisticRegression(**best_params)\n",
    "\n",
    "    # Step 4: Perform Recursive Feature Elimination (RFE)\n",
    "    rfe = RFE(estimator=best_model, n_features_to_select=1)\n",
    "    X_rfe = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "    # Step 5: Train the final Logistic Regression model with the best parameters and selected features\n",
    "    best_model.fit(X_rfe, y_train)\n",
    "\n",
    "    # Step 6: Evaluate the model on the test set\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    y_pred = best_model.predict(X_test_rfe)\n",
    "\n",
    "    title = \"Learning Curves (Logistic Regression)\"  # Change the title accordingly\n",
    "    scoring = \"accuracy\"  # Change to your preferred scoring metric\n",
    "\n",
    "    # Assuming you have a function named plot_learning_curve\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,  # Number of cross-validation folds\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4, 10):\n",
    "    print(\"Year: \", i)\n",
    "    logistic_regression_model(feature_engineering_result2, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def svm_model(dataset, year): \n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    # Scale the features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    #scaler = RobustScaler()\n",
    "    #scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Define the hyperparameters and their possible values\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['auto', 'scale'], \n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    svc = SVC()\n",
    "    grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Step 2: Extract the best parameters from Grid Search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Step 3: Create an SVM classifier with the best parameters\n",
    "    best_model = SVC(**best_params)\n",
    "\n",
    "    # Step 4: Perform Recursive Feature Elimination (RFE) with a linear SVM\n",
    "    linear_svc = LinearSVC()\n",
    "    rfe = RFE(estimator=linear_svc, n_features_to_select=10)\n",
    "    X_rfe = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "    # Step 5: Train the final SVM model with the best parameters and selected features\n",
    "    best_model.fit(X_rfe, y_train)\n",
    "\n",
    "    # Step 6: Evaluate the model on the test set\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    y_pred = best_model.predict(X_test_rfe)\n",
    "\n",
    "    title = \"Learning Curves (SVM)\"  # Change the title accordingly\n",
    "    scoring = \"accuracy\"  # Change to your preferred scoring metric\n",
    "\n",
    "    # Assuming you have a function named plot_learning_curve\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,  # Number of cross-validation folds\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def knn_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    # Scale the features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    #scaler = RobustScaler()\n",
    "    #scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Use SelectKBest for feature selection\n",
    "    k_best_selector = SelectKBest(f_classif, k='all')  # You can adjust 'k' based on your preference\n",
    "    X_train_selected = k_best_selector.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "    # Define the hyperparameters and their possible values\n",
    "    param_grid = {\n",
    "        'n_neighbors': [1, 3, 5, 7, 10],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=knn_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Extract the best parameters from Grid Search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Create the final k-Nearest Neighbors classifier with the best parameters\n",
    "    best_model = KNeighborsClassifier(**best_params)\n",
    "\n",
    "    # Train the final model with the selected features\n",
    "    best_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Evaluate the model on the scaled test set\n",
    "    X_test_selected = k_best_selector.transform(X_test_scaled)\n",
    "    y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "    title = \"Learning Curves (KNN)\"  # Change the title accordingly\n",
    "    scoring = \"accuracy\"  # Change to your preferred scoring metric\n",
    "\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train_selected,\n",
    "        y_train,\n",
    "        cv=5,  # Number of cross-validation folds\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gradient_boosting_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    # Define the hyperparameters and their possible values\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 1.0],\n",
    "    }\n",
    "\n",
    "    # Create a GradientBoostingClassifier object\n",
    "    gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "    # Create an RFECV object\n",
    "    rfecv = RFECV(estimator=gb_classifier, step=1, cv=5)  # You can adjust 'cv' based on your preference\n",
    "\n",
    "    # Perform RFECV on the training data\n",
    "    X_train_rfecv = rfecv.fit_transform(X_train, y_train)\n",
    "\n",
    "    # Extract the optimal number of features\n",
    "    optimal_num_features = rfecv.n_features_\n",
    "\n",
    "    print(f\"Optimal number of features: {optimal_num_features}\")\n",
    "\n",
    "    # Create a new GradientBoostingClassifier with the best parameters\n",
    "    grid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_rfecv, y_train)\n",
    "\n",
    "    # Extract the best parameters from Grid Search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Create the final Gradient Boosting classifier with the best parameters\n",
    "    best_model = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "    # Train the final model with the optimal features\n",
    "    best_model.fit(X_train_rfecv, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    X_test_rfecv = rfecv.transform(X_test)\n",
    "    y_pred = best_model.predict(X_test_rfecv)\n",
    "\n",
    "    title = \"Learning Curves (Gradient Boosting)\"  # Change the title accordingly\n",
    "    scoring = \"accuracy\"  # Change to your preferred scoring metric\n",
    "\n",
    "    # Assuming you have a function named plot_learning_curve\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train_rfecv,\n",
    "        y_train,\n",
    "        cv=5,  # Number of cross-validation folds\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    decision_tree_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    random_forest_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    logistic_regression_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    svm_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    knn_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    gradient_boosting_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Final Dataset (Year 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_decision_tree(dataset_train, dataset_predict):\n",
    "    X = dataset_train.copy()\n",
    "    y = dataset_train['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    X_test = dataset_predict\n",
    "    #y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    # Define the hyperparameters and their possible values\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 2, 3, 4, 5, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    dt_classifier = DecisionTreeClassifier()\n",
    "    grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Step 2: Extract the best parameters from Grid Search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Step 3: Create a Decision Tree classifier with the best parameters\n",
    "    best_model = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "    # Step 4: Perform Recursive Feature Elimination (RFE)\n",
    "    rfe = RFE(estimator=best_model, n_features_to_select=1)\n",
    "    X_rfe = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "    # Step 5: Train the final Decision Tree model with the best parameters and selected features\n",
    "    best_model.fit(X_rfe, y_train)\n",
    "\n",
    "    # Step 6: Evaluate the model on the test set\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    y_pred = best_model.predict(X_test_rfe)\n",
    "\n",
    "    # Display results\n",
    "    results_df = pd.DataFrame({'Team ID': X_test['tmID'], 'Predicted Playoffs': y_pred})\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = feature_engineering_result2.columns.to_list()\n",
    "selected_features.remove('playoff')\n",
    "selected_features.remove('coachesPowerRanking')\n",
    "\n",
    "feature_engineering_dataset = pd.read_csv('filtered/feature_engineering_dataset.csv', delimiter=\",\")\n",
    "feature_engineering_dataset.drop(columns=['playoff', 'coachesPowerRanking'], inplace=True)\n",
    "\n",
    "testing_data = pd.read_csv('filtered/team2_before_shift.csv', delimiter=\",\")\n",
    "testing_data.drop(columns=['playoff'], inplace=True)\n",
    "testing_data = pd.merge(testing_data, feature_engineering_dataset, on=['tmID', 'year'])\n",
    "testing_data = testing_data[selected_features]\n",
    "testing_data = testing_data[testing_data['year'] == 10]\n",
    "\n",
    "testing_data.to_csv('filtered/testing_data.csv', index=False)\n",
    "\n",
    "training_data = feature_engineering_result2.copy().drop(columns=['coachesPowerRanking'])\n",
    "\n",
    "final_decision_tree(training_data, testing_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

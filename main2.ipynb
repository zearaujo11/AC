{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of teams that will reach the Playoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "! pip install tabulate\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "players = pd.read_csv('basketballPlayoffs/players.csv', delimiter=\",\")\n",
    "coaches = pd.read_csv('basketballPlayoffs/coaches.csv', delimiter=\",\")\n",
    "teams = pd.read_csv('basketballPlayoffs/teams.csv', delimiter=\",\")\n",
    "players_teams = pd.read_csv('basketballPlayoffs/players_teams.csv', delimiter=\",\")\n",
    "teams_post = pd.read_csv('basketballPlayoffs/teams_post.csv', delimiter=\",\")\n",
    "series_post = pd.read_csv('basketballPlayoffs/series_post.csv', delimiter=\",\")\n",
    "awards_players = pd.read_csv('basketballPlayoffs/awards_players.csv', delimiter=\",\")\n",
    "awards_coaches = pd.read_csv('basketballPlayoffs/awards_coaches.csv', delimiter=\",\")\n",
    "\n",
    "print(players.head())\n",
    "print(coaches)\n",
    "print(teams)\n",
    "print(players_teams)\n",
    "print(teams_post)\n",
    "print(series_post)\n",
    "print(awards_players)\n",
    "print(awards_coaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values, outliers, and inconsistencies in the data. Clean and preprocess the data to ensure it's ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams.isna().sum())\n",
    "print(teams.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players Teams dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players_teams.isna().sum())\n",
    "print(players_teams.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players.isna().sum())\n",
    "print(players.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coaches dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coaches.isna().sum())\n",
    "print(coaches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awards Players dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(awards_players.isna().sum())\n",
    "print(awards_players.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awards Coaches dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(awards_coaches.isna().sum())\n",
    "print(awards_coaches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct EDA to gain insights into the data. Visualize distributions, correlations, and patterns. This step will help you understand the relationships between different features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams.head()\n",
    "\n",
    "teams.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in teams.columns:\n",
    "    unique_values = teams[column].unique()\n",
    "    print(f\"Number of different values in the {column} column are:\", len(unique_values))\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "value_counts = []\n",
    "\n",
    "for column in teams.columns:\n",
    "    unique_values = teams[column].nunique()\n",
    "    columns.append(column)\n",
    "    value_counts.append(unique_values)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(columns, value_counts, color='skyblue')\n",
    "plt.xlabel('Number of Unique Values')\n",
    "plt.ylabel('Columns')\n",
    "plt.title('Number of Unique Values in Each Column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_numeric = teams.copy()\n",
    "\n",
    "for column in teams_numeric.columns:\n",
    "    if teams_numeric[column].dtype == 'object':\n",
    "        teams_numeric[column] = teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "teams_numeric.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(dataframe):\n",
    "    corr_matrix = dataframe.corr()\n",
    "\n",
    "    target_correlation = corr_matrix['playoff']\n",
    "\n",
    "    plt.figure(figsize=(30, 20))\n",
    "\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, annot_kws={\"size\": 8}, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
    "\n",
    "    plt.title('Correlation Matrix', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    dict = {}\n",
    "\n",
    "    for feature, correlation in target_correlation.items():\n",
    "        print(f\"Correlation between target and {feature}: {correlation}\")\n",
    "        dict[feature] = correlation\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(teams_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square(dataset, target): \n",
    "    for feature in dataset.columns:\n",
    "        if feature != target:\n",
    "            contingency_table = pd.crosstab(dataset[feature], dataset[target])\n",
    "\n",
    "            # check if any category has no data\n",
    "            if contingency_table.shape[0] == 0 or contingency_table.shape[1] == 0:\n",
    "                print(f\"No data for {feature} and {target}\")\n",
    "                continue\n",
    "            \n",
    "            chi2, p, observed, expected = chi2_contingency(contingency_table)\n",
    "            \n",
    "            # Step 4: Print or store the results\n",
    "            print(f\"Chi-square test for {feature} and {target}:\")\n",
    "            print(f\"Chi-square value: {chi2}\")\n",
    "            print(f\"P-value: {p}\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square(teams, 'playoff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value indicates the probability of observing a relationship as extreme as the one in our sample data, assuming that there is no actual relationship in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players['playerID'].nunique()) \n",
    "\n",
    "print(players.head())\n",
    "\n",
    "players.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erased columns and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped columns: 'collegeOther', 'deathDate', 'firstseason', 'lastseason'\n",
    "\n",
    "# Count the rows where 'firstseason' is not equal to 0\n",
    "non_zero_firstseason_count = len(players[players['firstseason'] != 0])\n",
    "\n",
    "# Count the rows where 'firstseason' is not equal to 0\n",
    "non_zero_lastseason_count = len(players[players['lastseason'] != 0])\n",
    "\n",
    "# Count the rows where 'deathDate' is not equal to \"0000-00-00\"\n",
    "players['deathDate'] = players['deathDate'].str.strip()\n",
    "non_empty_deathDate_count = len(players[players['deathDate'] != \"0000-00-00\"])\n",
    "\n",
    "# Count the rows where 'collegeOther' is not equal to \"\"\n",
    "non_nan_collegeOther_count = players['collegeOther'].notna().sum()\n",
    "\n",
    "print(\"Number of rows with 'firstseason' different from 0:\", non_zero_firstseason_count)\n",
    "print(\"Number of rows with 'lastseason' different from 0:\", non_zero_lastseason_count)\n",
    "print(\"Number of rows with 'collegeOther' different from \"\":\", non_nan_collegeOther_count)\n",
    "print(\"Number of rows with 'deathDate' different from '0000-00-00':\", non_empty_deathDate_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Players heights comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert height from inches to centimeters\n",
    "players['height_cm'] = players['height'] * 2.54  # 1 inch = 2.54 cm\n",
    "\n",
    "# Define height categories in centimeters\n",
    "height_categories = ['< 160.0 cm', '160.0 - 170.0 cm', '170.0 - 180.0 cm', '180.0 - 190.0 cm', '190.0 - 200.0 cm', '> 200.0 cm']\n",
    "\n",
    "# Define the height ranges for each category\n",
    "height_ranges = [(0, 160.0), (160.0, 170.0), (170.0, 180.0), (180.0, 190.0), (190.0, 200.0), (200.0, float('inf'))]\n",
    "\n",
    "# Create a new column in the dataset to store the height category for each player\n",
    "players['height_category'] = pd.cut(players['height_cm'], bins=[r[0] for r in height_ranges] + [float('inf')], labels=height_categories)\n",
    "\n",
    "# Count the number of players in each height category\n",
    "height_category_counts = players['height_category'].value_counts().reindex(height_categories, fill_value=0)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=height_category_counts.index, y=height_category_counts.values, palette='Set2')\n",
    "\n",
    "# Add labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Player Count in Height Categories')\n",
    "plt.xlabel('Height Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of players in each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot for player positions\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "ax = sns.countplot(data=players, x='pos', order=players['pos'].value_counts().index, palette='Set2')\n",
    "\n",
    "# Add labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Number of Players in Each Position')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Rotate x-axis labels for better readability (optional)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "empty_pos_count = players['pos'].isnull().sum()\n",
    "print(\"Number of rows with empty 'pos':\", empty_pos_count)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 Colleges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 colleges with the most players\n",
    "top_10_colleges = players['college'].value_counts().iloc[:10]\n",
    "\n",
    "# Create a countplot for the top 10 colleges\n",
    "plt.figure(figsize=(12, 6))  # Adjust the figure size as needed\n",
    "ax = sns.countplot(data=players, x='college', order=top_10_colleges.index, palette='Set2')\n",
    "\n",
    "# Add labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Top 10 Colleges with the Most Players')\n",
    "plt.xlabel('College')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Rotate x-axis labels for better readability (optional)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix between numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns for correlation analysis\n",
    "numeric_columns = [\"firstseason\", \"lastseason\", \"height\", \"weight\"]\n",
    "\n",
    "# Create a subset of the dataset with only the numeric columns\n",
    "subset = players[numeric_columns]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = subset.corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams Post metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams_post.head())\n",
    "\n",
    "teams_post.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate win-loss ratios\n",
    "teams_post['Win-Loss Ratio'] = teams_post['W'] / (teams_post['W'] + teams_post['L'])\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(teams_post['tmID'], teams_post['Win-Loss Ratio'], color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Win-Loss Ratio')\n",
    "plt.ylabel('Team ID (tmID)')\n",
    "plt.title('Win-Loss Ratios for Teams on  Post-Season (based on tmID)')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Post metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by year and count the number of wins for each team in each year\n",
    "team_wins_by_year = series_post.groupby(['year', 'tmIDWinner'])['W'].count().unstack(fill_value=0)\n",
    "\n",
    "# Create a stacked bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "team_wins_by_year.plot(kind='bar', stacked=True, colormap='Set3')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Wins')\n",
    "plt.title('Teams That Won In The Playoffs Each Year')\n",
    "\n",
    "# Show the chart\n",
    "plt.legend(title='Team', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teams that won and lost each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the \"F\" (Finals) round\n",
    "finals_data = series_post[series_post['round'] == 'F']\n",
    "\n",
    "# Create a DataFrame with the winning and losing teams for each year\n",
    "finals_results = finals_data[['year', 'tmIDWinner', 'tmIDLoser']]\n",
    "\n",
    "# Convert the DataFrame to a prettily formatted table\n",
    "table = tabulate(finals_results, headers='keys', tablefmt='fancy_grid', showindex=False)\n",
    "\n",
    "# Display the formatted table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the \"F\" (Finals) round\n",
    "finals_data = series_post[series_post['round'] == 'F']\n",
    "\n",
    "# Count how many times each team has appeared in the Finals as either a winner or a loser\n",
    "team_appearances = pd.concat([finals_data['tmIDWinner'], finals_data['tmIDLoser']]).value_counts()\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "team_appearances.plot(kind='barh', color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Team (tmID)')\n",
    "plt.xlabel('Number of Finals Appearances')\n",
    "plt.title('Total Appearances of Each Team in the Finals')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coaches metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coaches['coachID'].nunique()) \n",
    "\n",
    "print(coaches.head())\n",
    "\n",
    "coaches.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the wins and losses data from the \"won\" and \"lost\" columns\n",
    "coach_wins = coaches['won']\n",
    "coach_losses = coaches['lost']\n",
    "\n",
    "# Create a scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(coach_wins, coach_losses, alpha=0.5)\n",
    "plt.xlabel('Wins')\n",
    "plt.ylabel('Losses')\n",
    "plt.title('Scatter Plot of Coach Wins vs. Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify and select relevant features for your prediction model. Use techniques such as correlation analysis, recursive feature elimination, or feature importance from tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_teams = teams.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete: lgID, divID, seeded, tmORB, tmDRB, tmTRB, opptmORB, opptmDRB, opptmTRB, rank, firstRound, semis, finals\n",
    "print(teams.isna().sum())\n",
    "   \n",
    "feature_selection_result = teams.drop(columns=['lgID', 'divID', 'seeded', 'tmORB', 'tmDRB', 'tmTRB', 'opptmORB', 'opptmDRB', 'opptmTRB'])\n",
    "feature_selection_result = feature_selection_result.drop(columns=['rank', 'firstRound', 'semis', 'finals'])\n",
    "feature_selection_result = feature_selection_result.drop(columns=['min', 'o_oreb', 'o_dreb', 'd_oreb', 'd_dreb', 'name', 'franchID'])\n",
    "\n",
    "feature_selection_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_result.to_csv('filtered/feature_selection_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new features that might enhance the predictive power of your model. This could involve transforming existing features, creating interaction terms, or incorporating external data.\n",
    "\n",
    "The feauture enginnering is done inside the `players.ipynb` file and the creation of the new dataset is done inside `create_final_team.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_target_variable(dataset):\n",
    "    dataset.sort_values(by=['tmID', 'year'], inplace=True)\n",
    "\n",
    "    dataset['playoffs'] = dataset.groupby('tmID')['playoff'].shift(-1)\n",
    "\n",
    "    dataset.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    dataset.dropna(subset=['playoffs'], inplace=True)\n",
    "\n",
    "    dataset.rename(columns={'playoffs': 'playoff'}, inplace=True)\n",
    "\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_teams = shift_target_variable(original_teams)\n",
    "original_teams.to_csv('filtered/original_teams.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_teams_numeric = original_teams.copy()\n",
    "\n",
    "for column in original_teams_numeric.columns:\n",
    "    if original_teams_numeric[column].dtype == 'object':\n",
    "        original_teams_numeric[column] = original_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(original_teams_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_result = shift_target_variable(feature_selection_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_teams_numeric = feature_selection_result.copy()\n",
    "\n",
    "for column in feature_teams_numeric.columns:\n",
    "    if feature_teams_numeric[column].dtype == 'object':\n",
    "        feature_teams_numeric[column] = feature_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(feature_teams_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: verificar se pode ficar assim ou se adicionamos o código cá\n",
    "feature_engineering_result = pd.read_csv('filtered/feature_engineering_dataset.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng1_teams_numeric = shift_target_variable(feature_engineering_result.copy())\n",
    "eng1_teams_numeric.to_csv('filtered/eng1_teams_numeric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng1_teams_numeric = eng1_teams_numeric.copy()\n",
    "\n",
    "for column in eng1_teams_numeric.columns:\n",
    "    if eng1_teams_numeric[column].dtype == 'object':\n",
    "        eng1_teams_numeric[column] = eng1_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(eng1_teams_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_result2 = pd.read_csv('filtered/team2_before_shift.csv', delimiter=\",\")\n",
    "feature_engineering_result2.drop(columns=['playoff'], inplace=True)\n",
    "copy_fe1 = feature_engineering_result.copy()\n",
    "copy_fe1.drop(columns=['powerRanking2'], inplace=True)\n",
    "\n",
    "feature_engineering_result2 = pd.merge(feature_engineering_result2, copy_fe1, on=['tmID', 'year'])\n",
    "#feature_engineering_result2.to_csv('text.csv', index=False)\n",
    "feature_engineering_result2 = shift_target_variable(feature_engineering_result2)\n",
    "feature_engineering_result = shift_target_variable(feature_engineering_result)\n",
    "\n",
    "feature_engineering_result2.to_csv('text.csv', index=False)\n",
    "\n",
    "feature_engineering_result2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2_teams_numeric = feature_engineering_result2.copy()\n",
    "\n",
    "for column in eng2_teams_numeric.columns:\n",
    "    if eng2_teams_numeric[column].dtype == 'object':\n",
    "        eng2_teams_numeric[column] = eng2_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(eng2_teams_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = ['year', 'average_powerRanking', 'average_PER', 'average_postPowerRanking', 'average_postPER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2_teams_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmID, confID, playoff, arena\n",
    "# name, franchID, lgID, divID\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to the columns 'teamID', 'franchID', 'confID', 'name', 'arena'\n",
    "original_teams['tmID'] = label_encoder.fit_transform(original_teams['tmID'])\n",
    "original_teams['confID'] = label_encoder.fit_transform(original_teams['confID'])\n",
    "original_teams['arena'] = label_encoder.fit_transform(original_teams['arena'])\n",
    "original_teams['name'] = label_encoder.fit_transform(original_teams['name'])\n",
    "original_teams['franchID'] = label_encoder.fit_transform(original_teams['franchID'])\n",
    "original_teams['lgID'] = label_encoder.fit_transform(original_teams['lgID'])\n",
    "original_teams['divID'] = label_encoder.fit_transform(original_teams['divID'])\n",
    "original_teams['firstRound'] = label_encoder.fit_transform(original_teams['firstRound'])\n",
    "original_teams['semis'] = label_encoder.fit_transform(original_teams['semis'])\n",
    "original_teams['finals'] = label_encoder.fit_transform(original_teams['finals'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_result['tmID'] = label_encoder.fit_transform(feature_selection_result['tmID'])\n",
    "feature_selection_result['confID'] = label_encoder.fit_transform(feature_selection_result['confID'])\n",
    "feature_selection_result['arena'] = label_encoder.fit_transform(feature_selection_result['arena'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to the columns 'teamID'\n",
    "feature_engineering_result['tmID'] = label_encoder.fit_transform(feature_engineering_result['tmID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_result2['tmID'] = label_encoder.fit_transform(feature_engineering_result2['tmID'])\n",
    "feature_engineering_result2['confID'] = label_encoder.fit_transform(feature_engineering_result2['confID'])\n",
    "feature_engineering_result2.to_csv('text.csv', index=False)\n",
    "\"\"\" feature_engineering_result2['arena'] = label_encoder.fit_transform(feature_engineering_result2['arena'])\n",
    "feature_engineering_result2['firstRound'] = label_encoder.fit_transform(feature_engineering_result2['firstRound'])\n",
    "feature_engineering_result2['semis'] = label_encoder.fit_transform(feature_engineering_result2['semis'])\n",
    "feature_engineering_result2['finals'] = label_encoder.fit_transform(feature_engineering_result2['finals']) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, scoring=None, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=scoring, train_sizes=train_sizes, n_jobs=-1\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def decision_tree_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 2, 3, 4, 5, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    class_weights = {'N': 1, 'Y': 14}\n",
    "\n",
    "    dt_classifier = DecisionTreeClassifier(class_weight=class_weights)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "    min_features_to_select = 5 \n",
    "    step_value = 1 \n",
    "    rfe = RFECV(estimator=best_model, step=step_value, cv=5, min_features_to_select=min_features_to_select)\n",
    "    X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "    num_features_selected = rfe.n_features_\n",
    "    print(f\"Number of features selected: {num_features_selected}\")\n",
    "\n",
    "    selected_features = X_train.columns[rfe.support_]\n",
    "    print(\"Selected Features:\", selected_features)\n",
    "\n",
    "    best_model.fit(X_train_rfe, y_train)\n",
    "\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    y_pred = best_model.predict(X_test_rfe)\n",
    "\n",
    "    title = \"Learning Curves (Decision Tree)\"\n",
    "    scoring = \"accuracy\"\n",
    "\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train_rfe,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def random_forest_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [None, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 8, 12],\n",
    "        'min_samples_leaf': [1, 2, 4, 6], \n",
    "        'max_features': ['auto', 'sqrt', 'log2', None, 0.8, 0.9]\n",
    "    }\n",
    "\n",
    "    class_weights = {'N': 1, 'Y': 8}\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(class_weight=class_weights)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    title = \"Learning Curves (Random Forest)\"\n",
    "    scoring = \"accuracy\"\n",
    "\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4, 10): \n",
    "    print(\"Year: \", i)\n",
    "    random_forest_model(feature_engineering_result2, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1, 8): \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logistic_regression_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'lbfgs', 'saga'],\n",
    "        'max_iter': [1000, 10000]\n",
    "    }\n",
    "\n",
    "    lr_classifier = LogisticRegression()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=lr_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = LogisticRegression(**best_params)\n",
    "\n",
    "    # Remove RFECV step\n",
    "    X_train_rfe = X_train\n",
    "\n",
    "    best_model.fit(X_train_rfe, y_train)\n",
    "\n",
    "    X_test_rfe = X_test\n",
    "    y_pred = best_model.predict(X_test_rfe)\n",
    "\n",
    "    title = \"Learning Curves (Logistic Regression)\"\n",
    "    scoring = \"accuracy\"\n",
    "\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train_rfe,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    y_scores = best_model.predict_proba(X_test_rfe)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4, 10):\n",
    "    print(\"Year: \", i)\n",
    "    logistic_regression_model(feature_engineering_result2, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def svm_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['auto', 'scale'],\n",
    "    }\n",
    "\n",
    "    # Enable probability estimates\n",
    "    svc = SVC(probability=True)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = SVC(**best_params, class_weight={'Y': 5, 'N': 5}, probability=True)\n",
    "\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    title = \"Learning Curves (SVM)\"\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    # Use predict_proba for obtaining probability estimates\n",
    "    y_scores = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def knn_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    #scaler = RobustScaler()\n",
    "    #scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    k_best_selector = SelectKBest(f_classif, k='all')\n",
    "    X_train_selected = k_best_selector.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "    selected_features = X_train.columns[k_best_selector.get_support()]\n",
    "    print(\"Selected Features:\", selected_features)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_neighbors': [1, 3, 5, 7, 10],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=knn_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = KNeighborsClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    X_test_selected = k_best_selector.transform(X_test_scaled)\n",
    "    y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "    title = \"Learning Curves (KNN)\"\n",
    "    scoring = \"accuracy\"\n",
    "\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train_selected,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    y_scores = best_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gradient_boosting_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 1.0],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    title = \"Learning Curves (Gradient Boosting)\"\n",
    "    scoring = \"accuracy\"\n",
    "\n",
    "    # Assuming you have a plot_learning_curve function defined\n",
    "    plot_learning_curve(best_model, title, X_train, y_train, cv=5, scoring=scoring)\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    decision_tree_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    random_forest_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    logistic_regression_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    svm_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    knn_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    gradient_boosting_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Final Dataset (Year 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def final_decision_tree(dataset_train, dataset_predict):\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 2, 3, 4, 5, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    class_weights = {'N': 1, 'Y': 14}\n",
    "\n",
    "    dt_classifier = DecisionTreeClassifier(class_weight=class_weights)\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    results_df = pd.DataFrame({'Team ID': dataset_predict['tmID'], 'Predicted Playoffs': y_pred})\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def final_knn(dataset_train, dataset_predict):\n",
    "    # Extracting features and target variable for training set\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    # Extracting features for prediction set\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    #scaler = RobustScaler()\n",
    "    #scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    k_best_selector = SelectKBest(f_classif, k='all')\n",
    "    X_train_selected = k_best_selector.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "    selected_features = X_train.columns[k_best_selector.get_support()]\n",
    "    print(\"Selected Features:\", selected_features)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_neighbors': [1, 3, 5, 7, 10],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=knn_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = KNeighborsClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    X_test_selected = k_best_selector.transform(X_test_scaled)\n",
    "    y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "    results_df = pd.DataFrame({'Team ID': dataset_predict['tmID'], 'Predicted Playoffs': y_pred})\n",
    "    print(results_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def final_random_forest(dataset_train, dataset_predict):\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [None, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 8, 12],\n",
    "        'min_samples_leaf': [1, 2, 4, 6], \n",
    "        'max_features': ['auto', 'sqrt', 'log2', None, 0.8, 0.9]\n",
    "    }\n",
    "\n",
    "    class_weights = {'N': 1, 'Y': 8}\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(class_weight=class_weights)\n",
    "    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    results_df = pd.DataFrame({'Team ID': dataset_predict['tmID'], 'Predicted Playoffs': y_pred})\n",
    "    print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def final_logistic_regression(dataset_train, dataset_predict):\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'lbfgs', 'saga'],\n",
    "        'max_iter': [1000, 10000]\n",
    "    }\n",
    "\n",
    "    lr_classifier = LogisticRegression()\n",
    "    grid_search = GridSearchCV(estimator=lr_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = LogisticRegression(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    results_df = pd.DataFrame({'Team ID': dataset_predict['tmID'], 'Predicted Playoffs': y_pred})\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def final_svm(dataset_train, dataset_predict):\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['auto', 'scale'],\n",
    "    }\n",
    "\n",
    "    svc = SVC()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = SVC(**best_params, class_weight={'Y': 5, 'N': 5})\n",
    "\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    results_df = pd.DataFrame({'Team ID': dataset_predict['tmID'], 'Predicted Playoffs': y_pred})\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def final_gradient_boosting(dataset_train, dataset_predict):\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 1.0],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    results_df = pd.DataFrame({'Team ID': dataset_predict['tmID'], 'Predicted Playoffs': y_pred})\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = feature_engineering_result2.columns.to_list()\n",
    "selected_features.remove('playoff')\n",
    "print(selected_features)\n",
    "\n",
    "feature_engineering_dataset = pd.read_csv('filtered/feature_engineering_dataset.csv', delimiter=\",\")\n",
    "feature_engineering_dataset.drop(columns=['playoff', 'powerRanking2'], inplace=True)\n",
    "\n",
    "testing_data = pd.read_csv('filtered/team2_before_shift.csv', delimiter=\",\")\n",
    "#testing_data.to_csv('filtered/testing_data1.csv', index=False)\n",
    "testing_data = pd.merge(testing_data, feature_engineering_dataset, on=['tmID', 'year'])\n",
    "testing_data['tmID'] = label_encoder.fit_transform(testing_data['tmID'])\n",
    "testing_data['confID'] = label_encoder.fit_transform(testing_data['confID'])\n",
    "training_data = testing_data.copy()\n",
    "testing_data.drop(columns=['playoff'], inplace=True)\n",
    "testing_data = testing_data[selected_features]\n",
    "testing_data = testing_data[testing_data['year'] == 10]\n",
    "\n",
    "testing_data.to_csv('filtered/testing_data.csv', index=False)\n",
    "\n",
    "training_data = shift_target_variable(training_data)\n",
    "\n",
    "training_data.to_csv('filtered/training_data.csv', index=False)\n",
    "\n",
    "final_decision_tree(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_random_forest(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_logistic_regression(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_svm(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gradient_boosting(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_knn(training_data, testing_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

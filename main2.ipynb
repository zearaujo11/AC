{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of teams that will reach the Playoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Imports and Datasets loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "! pip install tabulate\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "players = pd.read_csv('basketballPlayoffs/players.csv', delimiter=\",\")\n",
    "coaches = pd.read_csv('basketballPlayoffs/coaches.csv', delimiter=\",\")\n",
    "teams = pd.read_csv('basketballPlayoffs/teams.csv', delimiter=\",\")\n",
    "players_teams = pd.read_csv('basketballPlayoffs/players_teams.csv', delimiter=\",\")\n",
    "teams_post = pd.read_csv('basketballPlayoffs/teams_post.csv', delimiter=\",\")\n",
    "series_post = pd.read_csv('basketballPlayoffs/series_post.csv', delimiter=\",\")\n",
    "awards_players = pd.read_csv('basketballPlayoffs/awards_players.csv', delimiter=\",\")\n",
    "awards_coaches = pd.read_csv('basketballPlayoffs/awards_coaches.csv', delimiter=\",\")\n",
    "\n",
    "print(players.head())\n",
    "print(coaches)\n",
    "print(teams)\n",
    "print(players_teams)\n",
    "print(teams_post)\n",
    "print(series_post)\n",
    "print(awards_players)\n",
    "print(awards_coaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Exploratory Data Analysis (EDA)\n",
    "\n",
    "### Overview\n",
    "This section focuses on explore each dataset to gain insights into the data. This step will help you understand the relationships between different features and the target variable. This process involves:\n",
    "\n",
    "#### **Teams**:\n",
    "\n",
    "1. **Checking for missing values**\n",
    "2. **Counting unique values per column**\n",
    "3. **Visualize distributions**\n",
    "4. **Visualize correlations and patterns**: Create a correlation matrix, ``correlation_matrix``, to evaluate the relationship between variables and performing a chi-square test, ``chi_square``, to evaluate the independence between each categorical feature and a specified target variable.\n",
    "\n",
    "#### **Players**:\n",
    "\n",
    "1.  **Checking for missing values**\n",
    "2.  **Counting unique values per column**\n",
    "3.  **Compare players height**: Create a graphic where heights are measure in cm and divided: ``<160.0``, ``160.0-170.0``, ``170.0-180.0``, ``180.0-190.0``, ``190.0-200.0``, ``>200.0``\n",
    "4.  **Visualize the number of players per position**: Create a graphic where we visualize the distribution of player position: ``G``, ``F``, ``C``, ``F-C``, ``G-F``, ``C-F``\n",
    "5.  **Visualize players top colleges**\n",
    "6.  **Visualize correlations and patterns**: Create a correlation matrix, ``correlation_matrix``, to evaluate the relationship between variables\n",
    "\n",
    "#### **Teams Post Season**\n",
    "\n",
    "1. **Checking for missing values**\n",
    "2. **Visualize win-loss ratios**: Calculate win-loss ratios with ``Win`` and ``Loss`` values, then create a ``Bar Chart`` to visualize the results\n",
    "\n",
    "#### **Series Post Season**\n",
    "\n",
    "1. **Visualize data insigths**:\n",
    "   - Teams that won in the playoffs each year\n",
    "   - Teams that won and lost each year\n",
    "   - Total appearances of each team in the finals\n",
    "\n",
    "#### **Coaches**\n",
    "\n",
    "1. **Checking for missing values**\n",
    "2. **Extract the wins and losses data**: Create a ``Scater Plot`` to visualize wins and losses data about coaches\n",
    "\n",
    "#### **Awards Players**\n",
    "1. **Checking for missing values**\n",
    "\n",
    "#### **Awards Coaches**\n",
    "1. **Checking for missing values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams.head()\n",
    "\n",
    "teams.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in teams.columns:\n",
    "    unique_values = teams[column].unique()\n",
    "    print(f\"Number of different values in the {column} column are:\", len(unique_values))\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "value_counts = []\n",
    "\n",
    "for column in teams.columns:\n",
    "    unique_values = teams[column].nunique()\n",
    "    columns.append(column)\n",
    "    value_counts.append(unique_values)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(columns, value_counts, color='skyblue')\n",
    "plt.xlabel('Number of Unique Values')\n",
    "plt.ylabel('Columns')\n",
    "plt.title('Number of Unique Values in Each Column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_numeric = teams.copy()\n",
    "\n",
    "for column in teams_numeric.columns:\n",
    "    if teams_numeric[column].dtype == 'object':\n",
    "        teams_numeric[column] = teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "teams_numeric.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(dataframe):\n",
    "    corr_matrix = dataframe.corr()\n",
    "\n",
    "    target_correlation = corr_matrix['playoff']\n",
    "\n",
    "    plt.figure(figsize=(30, 20))\n",
    "\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, annot_kws={\"size\": 8}, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
    "\n",
    "    plt.title('Correlation Matrix', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    dict = {}\n",
    "\n",
    "    for feature, correlation in target_correlation.items():\n",
    "        print(f\"Correlation between target and {feature}: {correlation}\")\n",
    "        dict[feature] = correlation\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(teams_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square(dataset, target): \n",
    "    for feature in dataset.columns:\n",
    "        if feature != target:\n",
    "            contingency_table = pd.crosstab(dataset[feature], dataset[target])\n",
    "\n",
    "            # check if any category has no data\n",
    "            if contingency_table.shape[0] == 0 or contingency_table.shape[1] == 0:\n",
    "                print(f\"No data for {feature} and {target}\")\n",
    "                continue\n",
    "            \n",
    "            chi2, p, observed, expected = chi2_contingency(contingency_table)\n",
    "            \n",
    "            # Step 4: Print or store the results\n",
    "            print(f\"Chi-square test for {feature} and {target}:\")\n",
    "            print(f\"Chi-square value: {chi2}\")\n",
    "            print(f\"P-value: {p}\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square(teams, 'playoff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value indicates the probability of observing a relationship as extreme as the one in our sample data, assuming that there is no actual relationship in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players['playerID'].nunique()) \n",
    "\n",
    "print(players.head())\n",
    "\n",
    "players.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the rows where 'firstseason' is not equal to 0\n",
    "non_zero_firstseason_count = len(players[players['firstseason'] != 0])\n",
    "\n",
    "# Count the rows where 'firstseason' is not equal to 0\n",
    "non_zero_lastseason_count = len(players[players['lastseason'] != 0])\n",
    "\n",
    "# Count the rows where 'deathDate' is not equal to \"0000-00-00\"\n",
    "players['deathDate'] = players['deathDate'].str.strip()\n",
    "non_empty_deathDate_count = len(players[players['deathDate'] != \"0000-00-00\"])\n",
    "\n",
    "# Count the rows where 'collegeOther' is not equal to \"\"\n",
    "non_nan_collegeOther_count = players['collegeOther'].notna().sum()\n",
    "\n",
    "print(\"Number of rows with 'firstseason' different from 0:\", non_zero_firstseason_count)\n",
    "print(\"Number of rows with 'lastseason' different from 0:\", non_zero_lastseason_count)\n",
    "print(\"Number of rows with 'collegeOther' different from \"\":\", non_nan_collegeOther_count)\n",
    "print(\"Number of rows with 'deathDate' different from '0000-00-00':\", non_empty_deathDate_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Players heights comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert height from inches to centimeters\n",
    "players['height_cm'] = players['height'] * 2.54  # 1 inch = 2.54 cm\n",
    "\n",
    "# Define height categories in centimeters\n",
    "height_categories = ['< 160.0 cm', '160.0 - 170.0 cm', '170.0 - 180.0 cm', '180.0 - 190.0 cm', '190.0 - 200.0 cm', '> 200.0 cm']\n",
    "\n",
    "# Define the height ranges for each category\n",
    "height_ranges = [(0, 160.0), (160.0, 170.0), (170.0, 180.0), (180.0, 190.0), (190.0, 200.0), (200.0, float('inf'))]\n",
    "\n",
    "# Create a new column in the dataset to store the height category for each player\n",
    "players['height_category'] = pd.cut(players['height_cm'], bins=[r[0] for r in height_ranges] + [float('inf')], labels=height_categories)\n",
    "\n",
    "# Count the number of players in each height category\n",
    "height_category_counts = players['height_category'].value_counts().reindex(height_categories, fill_value=0)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=height_category_counts.index, y=height_category_counts.values, palette='Set2')\n",
    "\n",
    "# Add labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Player Count in Height Categories')\n",
    "plt.xlabel('Height Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of players in each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot for player positions\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "ax = sns.countplot(data=players, x='pos', order=players['pos'].value_counts().index, palette='Set2')\n",
    "\n",
    "# Add labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Number of Players in Each Position')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Rotate x-axis labels for better readability (optional)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "empty_pos_count = players['pos'].isnull().sum()\n",
    "print(\"Number of rows with empty 'pos':\", empty_pos_count)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 Colleges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 colleges with the most players\n",
    "top_10_colleges = players['college'].value_counts().iloc[:10]\n",
    "\n",
    "# Create a countplot for the top 10 colleges\n",
    "plt.figure(figsize=(12, 6))  # Adjust the figure size as needed\n",
    "ax = sns.countplot(data=players, x='college', order=top_10_colleges.index, palette='Set2')\n",
    "\n",
    "# Add labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Top 10 Colleges with the Most Players')\n",
    "plt.xlabel('College')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Rotate x-axis labels for better readability (optional)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix between numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns for correlation analysis\n",
    "numeric_columns = [\"firstseason\", \"lastseason\", \"height\", \"weight\"]\n",
    "\n",
    "# Create a subset of the dataset with only the numeric columns\n",
    "subset = players[numeric_columns]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = subset.corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams Post metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teams_post.head())\n",
    "\n",
    "teams_post.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate win-loss ratios\n",
    "teams_post['Win-Loss Ratio'] = teams_post['W'] / (teams_post['W'] + teams_post['L'])\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(teams_post['tmID'], teams_post['Win-Loss Ratio'], color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Win-Loss Ratio')\n",
    "plt.ylabel('Team ID (tmID)')\n",
    "plt.title('Win-Loss Ratios for Teams on  Post-Season (based on tmID)')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Post metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by year and count the number of wins for each team in each year\n",
    "team_wins_by_year = series_post.groupby(['year', 'tmIDWinner'])['W'].count().unstack(fill_value=0)\n",
    "\n",
    "# Create a stacked bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "team_wins_by_year.plot(kind='bar', stacked=True, colormap='Set3')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Wins')\n",
    "plt.title('Teams That Won In The Playoffs Each Year')\n",
    "\n",
    "# Show the chart\n",
    "plt.legend(title='Team', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teams that won and lost each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the \"F\" (Finals) round\n",
    "finals_data = series_post[series_post['round'] == 'F']\n",
    "\n",
    "# Create a DataFrame with the winning and losing teams for each year\n",
    "finals_results = finals_data[['year', 'tmIDWinner', 'tmIDLoser']]\n",
    "\n",
    "# Convert the DataFrame to a prettily formatted table\n",
    "table = tabulate(finals_results, headers='keys', tablefmt='fancy_grid', showindex=False)\n",
    "\n",
    "# Display the formatted table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the \"F\" (Finals) round\n",
    "finals_data = series_post[series_post['round'] == 'F']\n",
    "\n",
    "# Count how many times each team has appeared in the Finals as either a winner or a loser\n",
    "team_appearances = pd.concat([finals_data['tmIDWinner'], finals_data['tmIDLoser']]).value_counts()\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "team_appearances.plot(kind='barh', color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Team (tmID)')\n",
    "plt.xlabel('Number of Finals Appearances')\n",
    "plt.title('Total Appearances of Each Team in the Finals')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coaches metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coaches['coachID'].nunique()) \n",
    "\n",
    "print(coaches.head())\n",
    "\n",
    "coaches.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the wins and losses data from the \"won\" and \"lost\" columns\n",
    "coach_wins = coaches['won']\n",
    "coach_losses = coaches['lost']\n",
    "\n",
    "# Create a scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(coach_wins, coach_losses, alpha=0.5)\n",
    "plt.xlabel('Wins')\n",
    "plt.ylabel('Losses')\n",
    "plt.title('Scatter Plot of Coach Wins vs. Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awards Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(awards_players.isna().sum())\n",
    "print(awards_players.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awards Coaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(awards_coaches.isna().sum())\n",
    "print(awards_coaches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Feature Selection\n",
    "This section focuses on identify and select relevant features for your prediction model. Use techniques such as correlation analysis, recursive feature elimination, or feature importance from tree-based models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_teams = teams.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete: lgID, divID, seeded, tmORB, tmDRB, tmTRB, opptmORB, opptmDRB, opptmTRB, rank, firstRound, semis, finals\n",
    "print(teams.isna().sum())\n",
    "   \n",
    "feature_selection_result = teams.drop(columns=['lgID', 'divID', 'seeded', 'tmORB', 'tmDRB', 'tmTRB', 'opptmORB', 'opptmDRB', 'opptmTRB'])\n",
    "feature_selection_result = feature_selection_result.drop(columns=['rank', 'firstRound', 'semis', 'finals'])\n",
    "feature_selection_result = feature_selection_result.drop(columns=['min', 'o_oreb', 'o_dreb', 'd_oreb', 'd_dreb', 'name', 'franchID'])\n",
    "\n",
    "feature_selection_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_result.to_csv('filtered/feature_selection_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Feature Engineering\n",
    "\n",
    "This section focuses on create new features that might enhance the predictive power of your model. This could involve transforming existing features, creating interaction terms, or incorporating external data.\n",
    "\n",
    "The feauture enginnering is done inside the `players.ipynb` file and the creation of the new dataset is done inside `create_final_team.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_target_variable(dataset):\n",
    "    dataset.sort_values(by=['tmID', 'year'], inplace=True)\n",
    "\n",
    "    dataset['playoffs'] = dataset.groupby('tmID')['playoff'].shift(-1)\n",
    "\n",
    "    dataset.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    dataset.dropna(subset=['playoffs'], inplace=True)\n",
    "\n",
    "    dataset.rename(columns={'playoffs': 'playoff'}, inplace=True)\n",
    "\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_teams = shift_target_variable(original_teams)\n",
    "original_teams.to_csv('filtered/original_teams.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_teams_numeric = original_teams.copy()\n",
    "\n",
    "for column in original_teams_numeric.columns:\n",
    "    if original_teams_numeric[column].dtype == 'object':\n",
    "        original_teams_numeric[column] = original_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(original_teams_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_result = shift_target_variable(feature_selection_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_teams_numeric = feature_selection_result.copy()\n",
    "\n",
    "for column in feature_teams_numeric.columns:\n",
    "    if feature_teams_numeric[column].dtype == 'object':\n",
    "        feature_teams_numeric[column] = feature_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(feature_teams_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: verificar se pode ficar assim ou se adicionamos o código cá\n",
    "feature_engineering_result = pd.read_csv('filtered/feature_engineering_dataset.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng1_teams_numeric = shift_target_variable(feature_engineering_result.copy())\n",
    "eng1_teams_numeric.to_csv('filtered/eng1_teams_numeric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng1_teams_numeric = eng1_teams_numeric.copy()\n",
    "\n",
    "for column in eng1_teams_numeric.columns:\n",
    "    if eng1_teams_numeric[column].dtype == 'object':\n",
    "        eng1_teams_numeric[column] = eng1_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(eng1_teams_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_result2 = pd.read_csv('filtered/team2_before_shift.csv', delimiter=\",\")\n",
    "feature_engineering_result2.drop(columns=['playoff'], inplace=True)\n",
    "copy_fe1 = feature_engineering_result.copy()\n",
    "copy_fe1.drop(columns=['powerRanking2'], inplace=True)\n",
    "\n",
    "feature_engineering_result2 = pd.merge(feature_engineering_result2, copy_fe1, on=['tmID', 'year'])\n",
    "#feature_engineering_result2.to_csv('text.csv', index=False)\n",
    "feature_engineering_result2 = shift_target_variable(feature_engineering_result2)\n",
    "feature_engineering_result = shift_target_variable(feature_engineering_result)\n",
    "\n",
    "feature_engineering_result2.to_csv('text.csv', index=False)\n",
    "\n",
    "feature_engineering_result2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2_teams_numeric = feature_engineering_result2.copy()\n",
    "\n",
    "for column in eng2_teams_numeric.columns:\n",
    "    if eng2_teams_numeric[column].dtype == 'object':\n",
    "        eng2_teams_numeric[column] = eng2_teams_numeric[column].astype('category').cat.codes\n",
    "\n",
    "correlation_matrix(eng2_teams_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = ['year', 'average_powerRanking', 'average_PER', 'average_postPowerRanking', 'average_postPER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2_teams_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Model Training and Classification\n",
    "This section focuses on create all models, training them on the data. The models created are ``Decision Tree``, ``Random Forest``, ``Logistic Regression``, ``Support Vector Machines``, ``K-Nearest Neighbors``, ``Gradient Boosting``. The process involves:\n",
    "\n",
    "- **Encoding**\n",
    "- **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Apply label enconding to non numerical values in ``Original``, ``Feature Selection``, ``Feature Engineering`` datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmID, confID, playoff, arena\n",
    "# name, franchID, lgID, divID\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to the columns 'teamID', 'franchID', 'confID', 'name', 'arena'\n",
    "original_teams['tmID'] = label_encoder.fit_transform(original_teams['tmID'])\n",
    "original_teams['confID'] = label_encoder.fit_transform(original_teams['confID'])\n",
    "original_teams['arena'] = label_encoder.fit_transform(original_teams['arena'])\n",
    "original_teams['name'] = label_encoder.fit_transform(original_teams['name'])\n",
    "original_teams['franchID'] = label_encoder.fit_transform(original_teams['franchID'])\n",
    "original_teams['lgID'] = label_encoder.fit_transform(original_teams['lgID'])\n",
    "original_teams['divID'] = label_encoder.fit_transform(original_teams['divID'])\n",
    "original_teams['firstRound'] = label_encoder.fit_transform(original_teams['firstRound'])\n",
    "original_teams['semis'] = label_encoder.fit_transform(original_teams['semis'])\n",
    "original_teams['finals'] = label_encoder.fit_transform(original_teams['finals'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_result['tmID'] = label_encoder.fit_transform(feature_selection_result['tmID'])\n",
    "feature_selection_result['confID'] = label_encoder.fit_transform(feature_selection_result['confID'])\n",
    "feature_selection_result['arena'] = label_encoder.fit_transform(feature_selection_result['arena'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to the columns 'teamID'\n",
    "feature_engineering_result['tmID'] = label_encoder.fit_transform(feature_engineering_result['tmID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_result2['tmID'] = label_encoder.fit_transform(feature_engineering_result2['tmID'])\n",
    "feature_engineering_result2['confID'] = label_encoder.fit_transform(feature_engineering_result2['confID'])\n",
    "feature_engineering_result2.to_csv('text.csv', index=False)\n",
    "\"\"\" feature_engineering_result2['arena'] = label_encoder.fit_transform(feature_engineering_result2['arena'])\n",
    "feature_engineering_result2['firstRound'] = label_encoder.fit_transform(feature_engineering_result2['firstRound'])\n",
    "feature_engineering_result2['semis'] = label_encoder.fit_transform(feature_engineering_result2['semis'])\n",
    "feature_engineering_result2['finals'] = label_encoder.fit_transform(feature_engineering_result2['finals']) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "- **Data Preparation**: Separates features (X) and the target variable (y) from the dataset, splits the data into training and testing sets\n",
    "\n",
    "- **Hyperparameter Tuning**: Uses GridSearchCV to perform a grid search cross-validation to find the best hyperparameters based on accuracy; Creates a classifier with the best parameters and fits it to the training data\n",
    "\n",
    "- **Prediction and Team Selection**: Predicts probabilities for the test set using the trained model; Sorts teams in each conference based on predicted probabilities and selects the top 4 teams from each conference\n",
    "\n",
    "- **Custom Binary Classification**: Creates a binary classification for playoff selection based on a threshold; Plots a learning curve for the random forest model on the training set\n",
    "\n",
    "- **ROC Curve and AUC**: Plots the ROC curve and calculates the Area Under the Curve (AUC) for the model on the test set\n",
    "\n",
    "- **Evaluation**: For each model, an evaluation is made for each dataset, using accuracy and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, scoring=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(scoring if scoring else 'Score')\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs,\n",
    "                                                            train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is divided into 5 folds, and the learning curve is generated by training and evaluating the model on these 5 folds. The purpose is to visualize how the model's performance changes with the size of the training set while considering different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the plot_learning_curve function defined\n",
    "# If not, you can use sklearn's plot_learning_curve or define your own\n",
    "\n",
    "def decision_tree_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 2, 3, 4, 5, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = DecisionTreeClassifier(random_state=42, **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Sort teams by predicted probabilities and get the indices\n",
    "    east_indices = np.argsort(y_scores[X_test['confID'] == '0'])[::-1][:4]\n",
    "    west_indices = np.argsort(y_scores[X_test['confID'] == '1'])[::-1][:4]\n",
    "\n",
    "    # Get the indices of the top 4 teams from each conference\n",
    "    selected_indices = np.concatenate([east_indices, west_indices])\n",
    "\n",
    "    # Create a binary classification based on threshold\n",
    "    y_pred_custom = np.zeros(len(X_test))\n",
    "    y_pred_custom[selected_indices] = 1\n",
    "\n",
    "    title = \"Learning Curves (Decision Tree)\"\n",
    "    scoring = \"accuracy\"\n",
    "\n",
    "    # Plot learning curve\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Convert labels to integers for classification_report\n",
    "    y_test_int = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    # Evaluate the custom predictions\n",
    "    accuracy = accuracy_score(y_test_int, y_pred_custom)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test_int, y_pred_custom, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the plot_learning_curve function defined\n",
    "# If not, you can use sklearn's plot_learning_curve or define your own\n",
    "\n",
    "def random_forest_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [None, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 8, 12],\n",
    "        'min_samples_leaf': [1, 2, 4, 6], \n",
    "        'max_features': ['auto', 'sqrt', 'log2', None, 0.8, 0.9]\n",
    "    }\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)#, class_weight=class_weights)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Sort teams by predicted probabilities and get the indices\n",
    "    east_indices = np.argsort(y_scores[X_test['confID'] == '0'])[::-1][:4]\n",
    "    west_indices = np.argsort(y_scores[X_test['confID'] == '1'])[::-1][:4]\n",
    "\n",
    "    # Get the indices of the top 4 teams from each conference\n",
    "    selected_indices = np.concatenate([east_indices, west_indices])\n",
    "\n",
    "    # Create a binary classification based on threshold\n",
    "    y_pred_custom = np.zeros(len(X_test))\n",
    "    y_pred_custom[selected_indices] = 1\n",
    "\n",
    "    title = \"Learning Curves (Random Forest)\"\n",
    "    scoring = \"accuracy\"\n",
    "\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Convert labels to integers for classification_report\n",
    "    y_test_int = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    # Evaluate the custom predictions\n",
    "    accuracy = accuracy_score(y_test_int, y_pred_custom)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test_int, y_pred_custom, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4, 10): \n",
    "    print(\"Year: \", i)\n",
    "    random_forest_model(feature_engineering_result2, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1, 8): \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logistic_regression_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'lbfgs', 'saga'],\n",
    "        'max_iter': [1000, 10000]\n",
    "    }\n",
    "\n",
    "    lr_classifier = LogisticRegression()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=lr_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = LogisticRegression(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    title = \"Learning Curves (Logistic Regression)\"\n",
    "    scoring = \"accuracy\"\n",
    "\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4, 10):\n",
    "    print(\"Year: \", i)\n",
    "    logistic_regression_model(feature_engineering_result2, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def svm_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['auto', 'scale'],\n",
    "    }\n",
    "\n",
    "    # Enable probability estimates\n",
    "    svc = SVC(probability=True)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = SVC(**best_params, class_weight={'Y': 5, 'N': 5}, probability=True)\n",
    "\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    title = \"Learning Curves (SVM)\"\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    # Use predict_proba for obtaining probability estimates\n",
    "    y_scores = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def knn_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    #scaler = RobustScaler()\n",
    "    #scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    k_best_selector = SelectKBest(f_classif, k='all')\n",
    "    X_train_selected = k_best_selector.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "    selected_features = X_train.columns[k_best_selector.get_support()]\n",
    "    print(\"Selected Features:\", selected_features)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_neighbors': [1, 3, 5, 7, 10],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=knn_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = KNeighborsClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    X_test_selected = k_best_selector.transform(X_test_scaled)\n",
    "    y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "    title = \"Learning Curves (KNN)\"\n",
    "    scoring = \"accuracy\"\n",
    "\n",
    "    plot_learning_curve(\n",
    "        best_model,\n",
    "        title,\n",
    "        X_train_selected,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=scoring,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    y_scores = best_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gradient_boosting_model(dataset, year):\n",
    "    X = dataset.copy()\n",
    "    y = dataset['playoff'].copy()\n",
    "    X.drop(columns=['playoff'], inplace=True)\n",
    "\n",
    "    X_train = dataset[dataset['year'] < year].drop(columns=['playoff'])\n",
    "    y_train = dataset[dataset['year'] < year]['playoff']\n",
    "\n",
    "    X_test = dataset[dataset['year'] == year].drop(columns=['playoff'])\n",
    "    y_test = dataset[dataset['year'] == year]['playoff']\n",
    "\n",
    "    gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 1.0],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    title = \"Learning Curves (Gradient Boosting)\"\n",
    "    scoring = \"accuracy\"\n",
    "\n",
    "    # Assuming you have a plot_learning_curve function defined\n",
    "    plot_learning_curve(best_model, title, X_train, y_train, cv=5, scoring=scoring)\n",
    "    plt.show()\n",
    "\n",
    "    y_test_binary = y_test.map({'N': 0, 'Y': 1})\n",
    "\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(original_teams, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(feature_selection_result, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(feature_engineering_result, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model(feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Model Evaluation\n",
    "This section focuses on observe how the models perfomance changes when trained on data from varying numbers of previous years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    decision_tree_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    random_forest_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    logistic_regression_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    svm_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    knn_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9): \n",
    "    print(\"Training year 9 based on previous \", i, \" years.\")\n",
    "    filtered_feature_engineering_result2 = feature_engineering_result2[feature_engineering_result2['year'] >= 9 - i]\n",
    "    gradient_boosting_model(filtered_feature_engineering_result2, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Testing for Final Dataset (Year 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def select_top_teams(predictions_df):\n",
    "    # Select the top 4 teams with the highest probabilities for each conference\n",
    "    selected_teams = predictions_df.groupby('confID').apply(lambda x: x.nlargest(4, 'Probability')).reset_index(drop=True)\n",
    "    return selected_teams\n",
    "\n",
    "def final_decision_tree(dataset_train, dataset_predict):\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 2, 3, 4, 5, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    class_weights = {'N': 1, 'Y': 14}\n",
    "\n",
    "    dt_classifier = DecisionTreeClassifier(class_weight=class_weights)\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities for each team\n",
    "    y_probabilities = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Combine predicted probabilities with team information\n",
    "    predictions_df = pd.DataFrame({'TeamID': X_test['tmID'], 'Probability': y_probabilities, 'confID': X_test['confID']})\n",
    "\n",
    "    # Select the top teams\n",
    "    selected_teams = select_top_teams(predictions_df)\n",
    "\n",
    "    # Display the selected teams\n",
    "    print(selected_teams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def select_top_teams(predictions_df):\n",
    "    # Select the top 4 teams with the highest probabilities for each conference\n",
    "    selected_teams = predictions_df.groupby('confID').apply(lambda x: x.nlargest(4, 'Probability')).reset_index(drop=True)\n",
    "    return selected_teams\n",
    "\n",
    "def final_random_forest(dataset_train, dataset_predict):\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [None, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 8, 12],\n",
    "        'min_samples_leaf': [1, 2, 4, 6], \n",
    "        'max_features': ['auto', 'sqrt', 'log2', None, 0.8, 0.9]\n",
    "    }\n",
    "\n",
    "    class_weights = {'N': 1, 'Y': 8}\n",
    "\n",
    "    rf_classifier = RandomForestClassifier()#class_weight=class_weights)\n",
    "    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities for each team\n",
    "    y_probabilities = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Combine predicted probabilities with team information\n",
    "    predictions_df = pd.DataFrame({'TeamID': X_test['tmID'], 'Probability': y_probabilities, 'confID': X_test['confID']})\n",
    "\n",
    "    # Select the top teams\n",
    "    selected_teams = select_top_teams(predictions_df)\n",
    "\n",
    "    # Display the selected teams\n",
    "    print(selected_teams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "def select_top_teams(predictions_df):\n",
    "    selected_teams = predictions_df.groupby('confID').apply(lambda x: x.nlargest(4, 'Probability')).reset_index(drop=True)\n",
    "    return selected_teams\n",
    "\n",
    "def final_logistic_regression(dataset_train, dataset_predict):\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    param_grid = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "\n",
    "    log_reg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = LogisticRegression(**best_params, solver='liblinear')\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_probabilities = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    predictions_df = pd.DataFrame({'TeamID': dataset_predict['tmID'], 'Probability': y_probabilities, 'confID': dataset_predict['confID']})\n",
    "\n",
    "    selected_teams = select_top_teams(predictions_df)\n",
    "\n",
    "    print(selected_teams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "def select_top_teams(predictions_df):\n",
    "    # Select the top 4 teams with the highest probabilities for each conference\n",
    "    selected_teams = predictions_df.groupby('confID').apply(lambda x: x.nlargest(4, 'Probability')).reset_index(drop=True)\n",
    "    return selected_teams\n",
    "\n",
    "def final_svm(dataset_train, dataset_predict):\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['auto', 'scale'],\n",
    "    }\n",
    "\n",
    "    svc = SVC()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = SVC(**best_params)\n",
    "\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict probabilities for each team\n",
    "    y_probabilities = best_model.decision_function(X_test_scaled)\n",
    "\n",
    "    # Combine predicted probabilities with team information\n",
    "    predictions_df = pd.DataFrame({'TeamID': X_test['tmID'], 'Probability': y_probabilities, 'confID': X_test['confID']})\n",
    "\n",
    "    # Select the top teams\n",
    "    selected_teams = select_top_teams(predictions_df)\n",
    "\n",
    "    # Display the selected teams\n",
    "    print(selected_teams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "def select_top_teams(predictions_df):\n",
    "    # Select the top teams with the highest probabilities for each conference\n",
    "    selected_teams = predictions_df.groupby('confID').apply(lambda x: x.nlargest(4, 'Probability')).reset_index(drop=True)\n",
    "    return selected_teams\n",
    "\n",
    "def final_knn(dataset_train, dataset_predict):\n",
    "    # Extracting features and target variable for training set\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    # Extracting features for prediction set\n",
    "    # Extracting features for prediction set\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    k_best_selector = SelectKBest(f_classif, k='all')\n",
    "    X_train_selected = k_best_selector.fit_transform(X_train_scaled, y_train)\n",
    "    X_test_selected = k_best_selector.transform(X_test_scaled)  # Fix this line\n",
    "\n",
    "    selected_features = X_train.columns[k_best_selector.get_support()]\n",
    "    print(\"Selected Features:\", selected_features)\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "        'n_neighbors': [1, 3, 5, 7, 10],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=knn_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = KNeighborsClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Predict probabilities for each team\n",
    "    y_probabilities = best_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "    # Combine predicted probabilities with team information\n",
    "    predictions_df = pd.DataFrame({'TeamID': X_test['tmID'], 'Probability': y_probabilities, 'confID': X_test['confID']})\n",
    "\n",
    "    # Select the top teams\n",
    "    selected_teams = select_top_teams(predictions_df)\n",
    "\n",
    "    # Display the selected teams\n",
    "    print(selected_teams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "def select_top_teams(predictions_df):\n",
    "    # Select the top 4 teams with the highest probabilities for each conference\n",
    "    selected_teams = predictions_df.groupby('confID').apply(lambda x: x.nlargest(4, 'Probability')).reset_index(drop=True)\n",
    "    return selected_teams\n",
    "\n",
    "def final_gradient_boosting(dataset_train, dataset_predict):\n",
    "    X_train = dataset_train.drop(columns=['playoff'])\n",
    "    y_train = dataset_train['playoff']\n",
    "\n",
    "    X_test = dataset_predict\n",
    "\n",
    "    gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 1.0],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    best_model = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities for each team\n",
    "    y_probabilities = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Combine predicted probabilities with team information\n",
    "    predictions_df = pd.DataFrame({'TeamID': X_test['tmID'], 'Probability': y_probabilities, 'confID': X_test['confID']})\n",
    "\n",
    "    # Select the top teams\n",
    "    selected_teams = select_top_teams(predictions_df)\n",
    "\n",
    "    # Display the selected teams\n",
    "    print(selected_teams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = feature_engineering_result2.columns.to_list()\n",
    "selected_features.remove('playoff')\n",
    "print(selected_features)\n",
    "\n",
    "feature_engineering_dataset = pd.read_csv('filtered/feature_engineering_dataset.csv', delimiter=\",\")\n",
    "feature_engineering_dataset.drop(columns=['playoff', 'powerRanking2'], inplace=True)\n",
    "\n",
    "testing_data = pd.read_csv('filtered/team2_before_shift.csv', delimiter=\",\")\n",
    "#testing_data.to_csv('filtered/testing_data1.csv', index=False)\n",
    "testing_data = pd.merge(testing_data, feature_engineering_dataset, on=['tmID', 'year'])\n",
    "testing_data['tmID'] = label_encoder.fit_transform(testing_data['tmID'])\n",
    "testing_data['confID'] = label_encoder.fit_transform(testing_data['confID'])\n",
    "training_data = testing_data.copy()\n",
    "testing_data.drop(columns=['playoff'], inplace=True)\n",
    "testing_data = testing_data[selected_features]\n",
    "testing_data = testing_data[testing_data['year'] == 10]\n",
    "\n",
    "testing_data.to_csv('filtered/testing_data.csv', index=False)\n",
    "\n",
    "training_data = shift_target_variable(training_data)\n",
    "\n",
    "training_data.to_csv('filtered/training_data.csv', index=False)\n",
    "\n",
    "final_decision_tree(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_random_forest(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_logistic_regression(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_svm(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gradient_boosting(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_knn(training_data, testing_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
